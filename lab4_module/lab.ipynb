{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123dbff4",
   "metadata": {},
   "source": [
    "# Лабораторна: Реалізація та Аналіз Дерев Рішень для Класифікації\n",
    "\n",
    "**Мета:** реалізувати дерево рішень з Gini, compute feature importance, implement pruning (cost-complexity), compare with sklearn DecisionTree & RandomForest, perform feature engineering and feature selection.\n",
    "\n",
    "_Всі кроки виконані у коді нижче — просто запустіть всі клітинки в Jupyter / Colab._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca89c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 0 — Імпорти та налаштування\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, copy, os, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, matthews_corrcoef, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "%matplotlib inline\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3936b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1 — Створення (або завантаження) датасету\n",
    "data_path = 'synthetic_coffee_health_10000.csv'\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"Loaded dataset from\", data_path, \"shape:\", df.shape)\n",
    "else:\n",
    "    print(\"Dataset not found. Generating synthetic dataset (10,000 samples) for demonstration.\")\n",
    "    np.random.seed(42)\n",
    "    N = 10000\n",
    "    ids = np.arange(1, N+1)\n",
    "    age = np.random.randint(18, 80, size=N)\n",
    "    gender = np.random.choice(['Male','Female','Other'], size=N, p=[0.48,0.48,0.04])\n",
    "    countries = np.random.choice(['USA','UK','France','Germany','India','Brazil','Kenya'], size=N)\n",
    "    occupation = np.random.choice(['Student','Employed','Healthcare','Retired','Unemployed','Other'], size=N)\n",
    "    coffee_intake = np.clip(np.round(np.random.gamma(2.0,1.2,size=N)), 0, 15)\n",
    "    caffeine_mg = coffee_intake * np.random.normal(95, 10, size=N)\n",
    "    sleep_hours = np.clip(np.random.normal(7, 1.5, size=N), 3, 10)\n",
    "    sleep_quality = np.random.choice(['Poor','Average','Good'], size=N, p=[0.2,0.5,0.3])\n",
    "    stress_level = np.random.choice(['Low','Medium','High'], size=N, p=[0.4,0.4,0.2])\n",
    "    smoking = np.random.choice(['Never','Former','Current'], size=N, p=[0.7,0.15,0.15])\n",
    "    alcohol = np.random.choice(['None','Low','Moderate','High'], size=N, p=[0.2,0.5,0.25,0.05])\n",
    "    bmi = np.round(np.random.normal(25,4,size=N),1)\n",
    "    heart_rate = np.round(np.random.normal(72,10,size=N)).astype(int)\n",
    "    physical_activity = np.clip(np.round(np.random.exponential(1.5,size=N),1), 0, 10)\n",
    "    score = (coffee_intake/5.0) - (sleep_hours/8.0) + (np.isin(stress_level, ['High']).astype(int)*0.8) +             (np.isin(smoking, ['Current']).astype(int)*0.6) + (np.isin(alcohol, ['High']).astype(int)*0.6) +             ((bmi-24)/10.0) - (physical_activity/5.0) + np.random.normal(0,0.5,size=N)\n",
    "    q = np.quantile(score, [0.25,0.5,0.75])\n",
    "    labels = np.where(score <= q[0], 'None', np.where(score <= q[1], 'Mild', np.where(score <= q[2], 'Moderate', 'Severe')))\n",
    "    df = pd.DataFrame({'ID': ids,'Age': age,'Gender': gender,'Country': countries,'Occupation': occupation,'Coffee_Intake': coffee_intake,'Caffeine_mg': np.round(caffeine_mg,1),'Sleep_Hours': np.round(sleep_hours,2),'Sleep_Quality': sleep_quality,'Stress_Level': stress_level,'Smoking': smoking,'Alcohol_Consumption': alcohol,'BMI': bmi,'Heart_Rate': heart_rate,'Physical_Activity_Hours': physical_activity,'Health_Issues': labels})\n",
    "print(\"Dataset ready — shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 2 — Попередня обробка та інженерія ознак\n",
    "X = df.drop(['ID','Health_Issues'], axis=1)\n",
    "y = df['Health_Issues']\n",
    "X['Coffee_Sleep_Interaction'] = X['Coffee_Intake'] * X['Sleep_Hours']\n",
    "X['Age_Binned'] = pd.cut(X['Age'], bins=[17,30,50,80], labels=['Young','Middle','Old'])\n",
    "cat_cols = ['Gender','Country','Occupation','Sleep_Quality','Stress_Level','Smoking','Alcohol_Consumption','Age_Binned']\n",
    "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "num_cols = ['Age','Coffee_Intake','Caffeine_mg','Sleep_Hours','BMI','Heart_Rate','Physical_Activity_Hours','Coffee_Sleep_Interaction']\n",
    "scaler = StandardScaler()\n",
    "X_encoded[num_cols] = scaler.fit_transform(X_encoded[num_cols])\n",
    "print(\"Preprocessing finished. Feature count:\", X_encoded.shape[1])\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 3 — Розбиття на train/val/test (60/20/20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_encoded, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "print(\"Train/Val/Test shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "le = LabelEncoder(); y_train_enc = le.fit_transform(y_train); y_val_enc = le.transform(y_val); y_test_enc = le.transform(y_test)\n",
    "class_names = le.classes_; print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 4 — Реалізація MyDecisionTree (Gini, feature importance)\n",
    "class MyDecisionTree:\n",
    "    def __init__(self, max_depth=6, min_samples=5):\n",
    "        self.max_depth = max_depth; self.min_samples = min_samples; self.feature_importance = None; self.tree = None; self.n_total = None\n",
    "    def gini(self, y):\n",
    "        if len(y) == 0: return 0.0\n",
    "        _, counts = np.unique(y, return_counts=True); p = counts / counts.sum(); return 1.0 - np.sum(p**2)\n",
    "    def best_split(self, X, y):\n",
    "        best_gini = float('inf'); best_feat=None; best_thr=None; n,m = X.shape\n",
    "        if n<=1: return None,None,None,None\n",
    "        parent_gini = self.gini(y)\n",
    "        for feature in range(m):\n",
    "            vals = np.unique(X[:,feature])\n",
    "            if len(vals)==1: continue\n",
    "            sorted_vals = np.sort(vals); thresholds = (sorted_vals[:-1]+sorted_vals[1:])/2.0\n",
    "            for thr in thresholds:\n",
    "                left_idx = X[:,feature] <= thr; right_idx = ~left_idx\n",
    "                if left_idx.sum() < self.min_samples or right_idx.sum() < self.min_samples: continue\n",
    "                g_left = self.gini(y[left_idx]); g_right = self.gini(y[right_idx])\n",
    "                g_weighted = (left_idx.sum()/n)*g_left + (right_idx.sum()/n)*g_right\n",
    "                if g_weighted < best_gini:\n",
    "                    best_gini = g_weighted; best_feat=feature; best_thr=thr\n",
    "        return best_feat, best_thr, best_gini, parent_gini\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        node={}; num_samples = len(y); values,counts = np.unique(y,return_counts=True); majority_class = int(values[np.argmax(counts)])\n",
    "        node['n_samples']=int(num_samples); node['class']=majority_class; node['gini']=float(self.gini(y))\n",
    "        if depth>=self.max_depth or num_samples<self.min_samples or node['gini']==0.0: node['leaf']=True; return node\n",
    "        feat,thr,best_gini,parent_gini = self.best_split(X,y)\n",
    "        if feat is None: node['leaf']=True; return node\n",
    "        node['leaf']=False; node['feature']=int(feat); node['threshold']=float(thr)\n",
    "        n=num_samples; left_mask = X[:,feat] <= thr; right_mask = ~left_mask\n",
    "        n_left = int(left_mask.sum()); n_right = int(right_mask.sum())\n",
    "        g_left = self.gini(y[left_mask]); g_right = self.gini(y[right_mask])\n",
    "        g_weighted = (n_left/n)*g_left + (n_right/n)*g_right\n",
    "        importance_contrib = (n / self.n_total) * (parent_gini - g_weighted); self.feature_importance[feat] += importance_contrib\n",
    "        node['left'] = self.build_tree(X[left_mask], y[left_mask], depth+1); node['right'] = self.build_tree(X[right_mask], y[right_mask], depth+1)\n",
    "        return node\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X, dtype=float); y = np.array(y, dtype=int); self.n_total = len(y)\n",
    "        self.feature_importance = np.zeros(X.shape[1], dtype=float); self.tree = self.build_tree(X,y,depth=0)\n",
    "        s = np.sum(self.feature_importance); \n",
    "        if s>0: self.feature_importance = self.feature_importance / s\n",
    "        return self\n",
    "    def predict_one(self, x, node):\n",
    "        if node.get('leaf',False): return int(node['class'])\n",
    "        if x[node['feature']] <= node['threshold']: return self.predict_one(x, node['left'])\n",
    "        else: return self.predict_one(x, node['right'])\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=float); return np.array([self.predict_one(x,self.tree) for x in X], dtype=int)\n",
    "    def count_leaves(self, node):\n",
    "        if node.get('leaf',False): return 1\n",
    "        return self.count_leaves(node['left']) + self.count_leaves(node['right'])\n",
    "    def calculate_error(self, X_val, y_val, node):\n",
    "        if len(y_val)==0: return 0.0\n",
    "        preds = np.full(len(y_val), node['class'], dtype=int); return np.sum(preds != y_val) / len(y_val)\n",
    "    def prune_tree(self, node, alpha, X_val, y_val):\n",
    "        if node.get('leaf',False): return self.calculate_error(X_val, y_val, node)\n",
    "        feat = node['feature']; thr = node['threshold']\n",
    "        left_mask = X_val[:, feat] <= thr; right_mask = ~left_mask\n",
    "        left_error = self.prune_tree(node['left'], alpha, X_val[left_mask], y_val[left_mask])\n",
    "        right_error = self.prune_tree(node['right'], alpha, X_val[right_mask], y_val[right_mask])\n",
    "        total_val = len(y_val)\n",
    "        if total_val>0:\n",
    "            def predict_subtree(x_row, node_local):\n",
    "                if node_local.get('leaf',False): return node_local['class']\n",
    "                if x_row[node_local['feature']] <= node_local['threshold']: return predict_subtree(x_row, node_local['left'])\n",
    "                else: return predict_subtree(x_row, node_local['right'])\n",
    "            preds = np.array([predict_subtree(x, node) for x in X_val])\n",
    "            subtree_mis = np.sum(preds != y_val) / total_val\n",
    "        else: subtree_mis = 0.0\n",
    "        leaf_error = self.calculate_error(X_val, y_val, {'leaf':True, 'class': node['class']}); leaves = self.count_leaves(node)\n",
    "        if (leaf_error + alpha) <= (subtree_mis + alpha * (leaves - 1)):\n",
    "            node['leaf'] = True; node.pop('left', None); node.pop('right', None); node.pop('feature', None); node.pop('threshold', None)\n",
    "            return leaf_error\n",
    "        else:\n",
    "            return subtree_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 5 — Навчання MyDecisionTree та оцінка\n",
    "tree = MyDecisionTree(max_depth=8, min_samples=10); tree.fit(X_train.values, y_train_enc)\n",
    "pred_train = tree.predict(X_train.values); pred_val = tree.predict(X_val.values); pred_test = tree.predict(X_test.values)\n",
    "acc_train = accuracy_score(y_train_enc, pred_train); acc_val = accuracy_score(y_val_enc, pred_val); acc_test = accuracy_score(y_test_enc, pred_test)\n",
    "f1_test = f1_score(y_test_enc, pred_test, average='macro'); mcc_test = matthews_corrcoef(y_test_enc, pred_test); cm_test = confusion_matrix(y_test_enc, pred_test)\n",
    "print(\"MyDecisionTree — before pruning: Train acc {:.3f}, Val acc {:.3f}, Test acc {:.3f}, Test F1 {:.3f}, MCC {:.3f}\".format(acc_train, acc_val, acc_test, f1_test, mcc_test))\n",
    "print(\"Confusion matrix (test):\\n\", cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 6 — Прунінг: пошук alpha за валідацією та застосування\n",
    "def prune_via_validation(tree_obj, X_val, y_val, alphas=[0.0,0.0005,0.001,0.002,0.005,0.01,0.02]):\n",
    "    best_alpha=None; best_err=1.0; best_tree=None\n",
    "    for a in alphas:\n",
    "        tree_copy = copy.deepcopy(tree_obj.tree); old_tree = tree_obj.tree; tree_obj.tree = tree_copy\n",
    "        tree_obj.prune_tree(tree_obj.tree, a, np.array(X_val), np.array(y_val))\n",
    "        preds_val = np.array([tree_obj.predict_one(x, tree_obj.tree) for x in np.array(X_val)])\n",
    "        val_err = np.sum(preds_val != np.array(y_val)) / len(y_val) if len(y_val)>0 else 0.0\n",
    "        if val_err < best_err: best_err = val_err; best_alpha=a; best_tree = copy.deepcopy(tree_obj.tree)\n",
    "        tree_obj.tree = old_tree\n",
    "    return best_alpha, best_err, best_tree\n",
    "best_alpha, best_err, best_tree = prune_via_validation(tree, X_val.values, y_val_enc)\n",
    "print(\"Best alpha:\", best_alpha, \"validation error:\", best_err)\n",
    "if best_tree is not None:\n",
    "    tree.tree = best_tree\n",
    "    pred_test_pruned = tree.predict(X_test.values)\n",
    "    acc_test_pruned = accuracy_score(y_test_enc, pred_test_pruned); f1_test_pruned = f1_score(y_test_enc, pred_test_pruned, average='macro'); mcc_test_pruned = matthews_corrcoef(y_test_enc, pred_test_pruned)\n",
    "    cm_test_pruned = confusion_matrix(y_test_enc, pred_test_pruned)\n",
    "    print(\"After pruning: Test acc {:.3f}, F1 {:.3f}, MCC {:.3f}\".format(acc_test_pruned, f1_test_pruned, mcc_test_pruned))\n",
    "else:\n",
    "    pred_test_pruned = pred_test; cm_test_pruned = cm_test; acc_test_pruned = acc_test; f1_test_pruned = f1_test; mcc_test_pruned = mcc_test\n",
    "    print(\"No pruning improvement found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 7 — Порівняння з sklearn DecisionTree та RandomForest\n",
    "dt_sk = DecisionTreeClassifier(criterion='gini', max_depth=8, min_samples_leaf=10, random_state=42); dt_sk.fit(X_train, y_train_enc); pred_dt_sk = dt_sk.predict(X_test)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42); rf.fit(X_train, y_train_enc); pred_rf = rf.predict(X_test)\n",
    "def compute_metrics(y_true, y_pred, name): return {'model':name, 'accuracy':accuracy_score(y_true,y_pred), 'f1_macro':f1_score(y_true,y_pred,average='macro'), 'mcc':matthews_corrcoef(y_true,y_pred)}\n",
    "metrics = [compute_metrics(y_test_enc, pred_test, 'MyDecisionTree (unpruned)'), compute_metrics(y_test_enc, pred_test_pruned, 'MyDecisionTree (pruned)'), compute_metrics(y_test_enc, pred_dt_sk, 'sklearn DecisionTree'), compute_metrics(y_test_enc, pred_rf, 'RandomForest')]\n",
    "metrics_df = pd.DataFrame(metrics); print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c591e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 8 — Візуалізації: confusion matrices та важливості ознак\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,4))\n",
    "axs[0].imshow(confusion_matrix(y_test_enc, pred_test), interpolation='nearest'); axs[0].set_title('MyTree (before prune)')\n",
    "axs[1].imshow(confusion_matrix(y_test_enc, pred_test_pruned), interpolation='nearest'); axs[1].set_title('MyTree (after prune)')\n",
    "axs[2].imshow(confusion_matrix(y_test_enc, pred_dt_sk), interpolation='nearest'); axs[2].set_title('sklearn DecisionTree')\n",
    "for ax in axs:\n",
    "    ax.set_xticks(range(len(class_names))); ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, rotation=45); ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "plt.tight_layout(); plt.show()\n",
    "feat_names = X_encoded.columns.tolist(); my_imp = tree.feature_importance; rf_imp = rf.feature_importances_\n",
    "top_idx_my = np.argsort(my_imp)[-15:][::-1]\n",
    "plt.figure(figsize=(12,4)); plt.barh([feat_names[i] for i in top_idx_my[::-1]], my_imp[top_idx_my[::-1]]); plt.title('Top features — MyDecisionTree importance (normalized)'); plt.xlabel('Importance'); plt.tight_layout(); plt.show()\n",
    "top_idx_rf = np.argsort(rf_imp)[-15:][::-1]; plt.figure(figsize=(12,4)); plt.barh([feat_names[i] for i in top_idx_rf[::-1]], rf_imp[top_idx_rf[::-1]]); plt.title('Top features — RandomForest importance'); plt.xlabel('Importance'); plt.tight_layout(); plt.show()\n",
    "perm = permutation_importance(rf, X_test, y_test_enc, n_repeats=5, random_state=42); perm_imp = perm.importances_mean; top_perm_idx = np.argsort(perm_imp)[-15:][::-1]\n",
    "plt.figure(figsize=(12,4)); plt.barh([feat_names[i] for i in top_perm_idx[::-1]], perm_imp[top_perm_idx[::-1]]); plt.title('Top features — permutation importance (RandomForest)'); plt.xlabel('Importance'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 9 — Feature selection за importance (top-10 від MyDecisionTree) та повторне навчання\n",
    "top10 = [feat_names[i] for i in np.argsort(my_imp)[-10:]]; print(\"Top-10 features by MyDecisionTree:\", top10)\n",
    "X_train_sel = X_train[top10]; X_test_sel = X_test[top10]\n",
    "dt_sk2 = DecisionTreeClassifier(criterion='gini', max_depth=8, min_samples_leaf=10, random_state=42); dt_sk2.fit(X_train_sel, y_train_enc); pred_dt2 = dt_sk2.predict(X_test_sel)\n",
    "rf2 = RandomForestClassifier(n_estimators=100, random_state=42); rf2.fit(X_train_sel, y_train_enc); pred_rf2 = rf2.predict(X_test_sel)\n",
    "metrics2 = [compute_metrics(y_test_enc, pred_dt2, 'sklearn DecisionTree (top10)'), compute_metrics(y_test_enc, pred_rf2, 'RandomForest (top10)')]; metrics2_df = pd.DataFrame(metrics2); print(metrics2_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 10 — Висновки (шаблон)\n",
    "print('--- ВИСНОВКИ ---') \n",
    "print('- Найкраща модель: (заповніть після запуску)') \n",
    "print('- Вплив прунінгу: (заповніть)') \n",
    "print('- Важливі ознаки: (перегляньте графіки)') \n",
    "print('- Рекомендації: (наприклад, додати cross-val, grid-search, tuning)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
